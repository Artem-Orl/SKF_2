{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import zipfile\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter\n",
    "# увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "# графики в svg выглядят более четкими\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "print('Keras        :', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Работаем с Tensorflow v2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В setup выносим основные настройки: так удобнее их перебирать в дальнейшем.\n",
    "\n",
    "EPOCHS = 15  # эпох на обучение.\n",
    "BATCH_SIZE = 40  # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR = 1e-4\n",
    "VAL_SPLIT = 0.15  # сколько данных выделяем на тест = 15%\n",
    "\n",
    "CLASS_NUM = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE = 224  # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS = 3   # у RGB 3 канала\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "PATH = \"../working/car/\"  # рабочая директория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Устаналиваем конкретное значение random seed для воспроизводимости\n",
    "os.makedirs(PATH, exist_ok=False)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Category.value_counts().plot(kind='barh')\n",
    "# распределение классов достаточно равномерное - это хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Распаковываем картинки')\n",
    "# Will unzip the files so that you can see them..\n",
    "for data_zip in ['train.zip', 'test.zip']:\n",
    "    with zipfile.ZipFile(\"../input/\"+data_zip, \"r\") as z:\n",
    "        z.extractall(PATH)\n",
    "\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print('Пример картинок (random sample)')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "random_image = train_df.sample(n=9)\n",
    "random_image_paths = random_image['Id'].values\n",
    "random_image_cat = random_image['Category'].values\n",
    "\n",
    "for index, path in enumerate(random_image_paths):\n",
    "    im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n",
    "    plt.subplot(3, 3, index+1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Class: '+str(random_image_cat[index]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры картинок и их размеры чтоб понимать как их лучше обработать и сжимать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(PATH+'/train/0/100380.jpg')\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(\n",
    "#rescale=1. / 255,\n",
    "#rotation_range = 5,\n",
    "# width_shift_range=0.1,\n",
    "# height_shift_range=0.1,\n",
    "# validation_split=VAL_SPLIT, # set validation split\n",
    "# horizontal_flip=False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем более продвинутый вариант аугментации данных\n",
    "# все делаем здесь для удобства проверки\n",
    "!pip install git+https://github.com/mjkvaak/ImageDataAugmentor\n",
    "    \n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations\n",
    "\n",
    "\n",
    "# сделаем более продвинутый генератор для train\n",
    "AUGMENTATION = albumentations.Compose(\n",
    "[\n",
    "    # повороты\n",
    "    albumentations.HorizontalFlip(p=0.25),\n",
    "    albumentations.ElasticTransform(p=0.25),\n",
    "    # цветовая аугментация\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HueSaturationValue(p=1.),\n",
    "        albumentations.RandomBrightnessContrast(p=1.),\n",
    "        albumentations.RGBShift(p=1.)\n",
    "    ], p=0.25),\n",
    "    # качество изображения\n",
    "    albumentations.OneOf([\n",
    "        albumentations.GaussNoise(p=1.,var_limit=(10.0, 50.0)),\n",
    "        albumentations.MultiplicativeNoise(p=1.),\n",
    "        albumentations.JpegCompression(p=1.),\n",
    "        albumentations.Downscale(scale_min=0.5,scale_max=0.99, p=1),\n",
    "    ], p=0.5)])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "        rescale=1./255,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        augment = AUGMENTATION\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками \n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training') # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "test_sub_generator = test_datagen.flow_from_dataframe( \n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)\n",
    "\n",
    "# Обратите внимание, что для сабмита мы используем другой источник test_datagen.flow_from_dataframe. Как вы думаете, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на примеры работы генератора\n",
    "x,y = train_generator.next()\n",
    "print('Пример картинок из test_generator')\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(0,9):\n",
    "    image = x[i]\n",
    "    plt.subplot(3,3, i+1)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем предобученную сеть FixEfficientNet-B7:\n",
    "не будем выносить вверх все установки и импорты  для удобства поиска проверяющего "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предложенная автором модель. Пока ее не используем\n",
    "\n",
    "#base_model = Xception(weights='imagenet', include_top=False, input_shape = input_shape)\n",
    "\n",
    "# Устанавливаем новую \"голову\" (head)\n",
    "\n",
    "#x = base_model.output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(256, activation='relu')(x)\n",
    "#x = Dropout(0.25)(x)\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "#predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "#model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель EfficientNetB7\n",
    "\n",
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "base_model = efn.EfficientNetB7(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на количество слоев в base_model\n",
    "print(len(base_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие модели\n",
    "### Вариант 1\n",
    "#### EfficientNetB7 без обучения  ее слоев (Голову оставим предложенную автором)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False\n",
    "#model=M.Sequential()\n",
    "#model.add(base_model)\n",
    "#model.add(L.GlobalAveragePooling2D())\n",
    "#model.add(L.Dense(256, activation='relu'))\n",
    "#model.add(L.Dropout(0.25))\n",
    "#model.add(L.Dense(CLASS_NUM, activation='softmax'))\n",
    "\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Остановили обучение на:\\\n",
    " train-loss: 1.3527 - accuracy: 0.5326 - val_loss: 1.1166 - val_accuracy: 0.6299\n",
    "\n",
    "\n",
    "### Вариант 2 (с Finetuning)\n",
    "#### EfficientNetB7 c заморозкой половины слоев (Голову оставим предложенную автором)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = True\n",
    "\n",
    "#fine_tuning_at= len(base_model.layers)//2\n",
    "\n",
    "# заморозим первые слои\n",
    "#for layer in base_model.layers[:fine_tuning_at]:\n",
    "  #layer.trainable =  False\n",
    "\n",
    "# проверим как все сработало:\n",
    "#for layer in base_model.layers:\n",
    "    #print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=M.Sequential()\n",
    "#model.add(base_model)\n",
    "#model.add(L.GlobalAveragePooling2D())\n",
    "#model.add(L.Dense(256, activation='relu'))\n",
    "#model.add(L.Dropout(0.25))\n",
    "#model.add(L.Dense(CLASS_NUM, activation='softmax'))\n",
    "\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "37/37 [==============================] - 54s 1s/step - loss: 0.2164 - accuracy: 0.9382\n",
    "Accuracy: 93.82%\n",
    "\n",
    "### Вариант 3 (с Finetuning)\n",
    "#### EfficientNetB7 c заморозкой 199 слоев (Голову оставим предложенную автором)\n",
    "#### также уменьшим размер батча до 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = True\n",
    "\n",
    "#fine_tuning_at= 200\n",
    "\n",
    "# заморозим первые слои\n",
    "#for layer in base_model.layers[:fine_tuning_at]:\n",
    "  #layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=M.Sequential()\n",
    "#model.add(base_model)\n",
    "#model.add(L.GlobalAveragePooling2D())\n",
    "#model.add(L.Dense(256, activation='relu'))\n",
    "#model.add(L.Dropout(0.25))\n",
    "#model.add(L.Dense(CLASS_NUM, activation='softmax'))\n",
    "\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "59/59 [==============================] - 59s 986ms/step - loss: 0.2109 - accuracy: 0.9446\n",
    "Accuracy: 94.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариант 4 (с Finetuning)\n",
    "#### EfficientNetB7 c заморозкой 199 слоев (Голову оставим предложенную автором)\n",
    "#### также уменьшим размер батча до 40\n",
    "#### добавим BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "fine_tuning_at= 200\n",
    "\n",
    "# заморозим первые слои\n",
    "for layer in base_model.layers[:fine_tuning_at]:\n",
    "  layer.trainable =  False\n",
    "    \n",
    "model=M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D())\n",
    "model.add(L.Dense(256, activation='relu'))\n",
    "model.add(L.BatchNormalization())\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "59/59 [==============================] - 56s 946ms/step - loss: 0.2039 - accuracy: 0.9459\n",
    "Accuracy: 94.59%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариант 5 (с Finetuning) (НЕ удалось получить прироста, скорее наоборот стало все гораздо хуже. Будем использовать модель 4)\n",
    "#### EfficientNetB7 c заморозкой 199 слоев (Голову оставим предложенную автором)\n",
    "#### также уменьшим размер батча до 40\n",
    "#### добавим BatchNormalization\n",
    "#### Попробуем поуправлять Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = True\n",
    "\n",
    "#fine_tuning_at= 200\n",
    "\n",
    "# заморозим первые слои\n",
    "#for layer in base_model.layers[:fine_tuning_at]:\n",
    "  #layer.trainable =  False\n",
    "    \n",
    "#model=M.Sequential()\n",
    "#model.add(base_model)\n",
    "#model.add(L.GlobalAveragePooling2D())\n",
    "#model.add(L.Dense(256, activation='relu'))\n",
    "#model.add(L.BatchNormalization())\n",
    "#model.add(L.Dropout(0.25))\n",
    "#model.add(L.Dense(CLASS_NUM, activation='softmax'))\n",
    "\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавим LearningRateScheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/bckenstler/CLR.git \n",
    "\n",
    "# from CLR.clr_callback import CyclicLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clr_step_size = int(8 * (len(train_generator)/BATCH_SIZE))\n",
    "# base_lr = 1e-4\n",
    "# max_lr = 1e-2\n",
    "# mode='triangular'\n",
    "\n",
    "# clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим ModelCheckpoint чтоб сохранять прогресс обучения модели и можно было потом подгрузить и дообучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best_model.hdf5' , monitor = ['val_accuracy'] , verbose = 1  , mode = 'max')\n",
    "# добавим также earlystop \n",
    "earlystopping=EarlyStopping(monitor=\"val_accuracy\", patience=5,restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,earlystopping]\n",
    "\n",
    "# попытка добавить цикличный learning rate\n",
    "#callbacks_list = [checkpoint,earlystopping,clr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = test_generator, \n",
    "        validation_steps = len(test_generator),\n",
    "        epochs = EPOCHS,\n",
    "        callbacks = callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n",
    "model.save('../working/model_last.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим графики обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.reset()\n",
    "predictions = model.predict_generator(test_sub_generator, steps=len(test_sub_generator), verbose=1) \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir=test_sub_generator.filenames\n",
    "submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/','')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Save submit')\n",
    "\n",
    "# Рекомендация: попробуйте добавить Test Time Augmentation (TTA)\n",
    "# https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Clean PATH\n",
    "import shutil\n",
    "shutil.rmtree(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы:\n",
    "Мы смогли увеличить результат модели с помощью:\n",
    "1. Продвинутой аугментации данных\n",
    "2. Выбора другой модели с использованием Finetuning и батч нормализации\n",
    "3. Увеличения количества эпох на обучение\n",
    "4. Манипуляции с learning rate не помогли, но возможно была использована неправильная реализация"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
